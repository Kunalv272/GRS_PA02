# 2-Copy, 1-Copy, and 0-Copy Socket Implementations

**Language:** C, Python  
**Platform:** Linux (x86_64)

---

## ğŸ“– Overview

This project experimentally evaluates the cost of data movement in network I/O by implementing and comparing three TCP-based clientâ€“server communication strategies:

| Implementation | Description         | Strategy |
|---------------|---------------------|----------|
| **A1** | Standard Baseline | Two-copy socket communication (`send` / `recv`) |
| **A2** | One-Copy Optimized | Scatterâ€“Gather I/O (`sendmsg` with `iovec`) |
| **A3** | Zero-Copy | Kernel-assisted zero-copy (`MSG_ZEROCOPY`) |

The goal is to measure and analyze both micro-architectural and application-level performance metrics, including throughput, latency, CPU cycles, and context switches.

---

## ğŸ“‚ Project Structure

.<br>
â”œâ”€â”€ Source Code<br>
â”‚ â”œâ”€â”€ a1_client.c / a1_server.c # Baseline implementation<br>
â”‚ â”œâ”€â”€ a2_client.c / a2_server.c # One-copy implementation<br>
â”‚ â”œâ”€â”€ a3_client.c / a3_server.c # Zero-copy implementation<br>
â”‚<br>
â”œâ”€â”€ Scripts<br>
â”‚ â”œâ”€â”€ Makefile # Build system<br>
â”‚ â”œâ”€â”€ MT25029_Part_C_RunExperiments.sh # Experiment automation<br>
â”‚ â”œâ”€â”€ MT25029_Part_D.py # Plotting script (Python)<br>
â”‚<br>
â””â”€â”€ Output (Generated)<br>
â”œâ”€â”€ MT25029_Raw.csv # Raw experimental data<br>
â””â”€â”€ *.png # Performance graphs<br>


---

## âš™ï¸ System Configuration

- **CPU:** Intel Core i7  
- **RAM:** 16 GB  
- **OS:** Linux (x86_64)  
- **Kernel:** Default Linux distribution kernel  
- **Network:** TCP over localhost (loopback)

---

## ğŸš€ Quick Start

### 1. Build the Project

Clean previous builds and compile all binaries:

```bash
make clean && make

2. Run Experiments (Part C + Part D)

Run the automation script to execute all benchmarks and generate plots:

./MT25029_Part_C_RunExperiments.sh

    Note: No manual intervention is required. The script handles compilation, execution, data collection, and plotting.

ğŸ“Š Methodology & Metrics
Metrics Collected

    Throughput: Application-level throughput (Mbps)

    Latency: Round-trip time (Âµs)

    CPU Cycles: Measured using perf stat

    Context Switches: Measured using perf stat

    Cache Behavior: Measured using perf stat

Experimental Parameters

    Message Sizes: 64 B, 512 B, 4096 B, 65536 B

    Thread Counts: 1, 2, 4, 8

ğŸ“ˆ Output Format
CSV Data (MT25029_Raw.csv)

Results are automatically appended in the following format:

impl,msg_size,threads,latency_us,throughput_mbps,cycles,context_switches

Generated Plots

After execution, the following plots are generated:

    throughput.png

    latency.png (logarithmic scale)

    context_switches.png

    cycles_per_byte.png

Each plot contains four subplots, corresponding to message sizes:

    64 B

    512 B

    4096 B

    65536 B

âš ï¸ Notes and Limitations

    Localhost Only: Experiments run on loopback; real network latency and jitter are not represented.

    Permissions: Access to cache counters and perf statistics may require lowering the kernel perf_event_paranoid level.

    Variability: Absolute performance values depend on hardware characteristics and OS scheduling state at runtime.

ğŸ“ Conclusion

This project demonstrates the performance impact of different socket communication strategies in Linux. By automating the comparison of A1 (two-copy), A2 (one-copy), and A3 (zero-copy) approaches, the experiments highlight how message size and thread count influence latency, throughput, and kernel scheduling overhead.

